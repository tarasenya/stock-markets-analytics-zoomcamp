{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit here:\n",
    "https://courses.datatalks.club/sma-zoomcamp-2024/homework/hw03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (1 point): Dummies on Month and Week-of-Month\n",
    "\n",
    "Find the ABSOLUTE CORRELATION VALUE of the most correlated dummy <month-week_of_month> with the binary outcome variable is_positive_growth_5d_future?\n",
    "\n",
    "You saw in the correlation analysis and modeling that September and October may be important seasonal months. In this task, we'll go futher and try to generate dummies for Month and Week-of-month (starting from 1). For example, the first week of October should be coded similar to this: 'October_w1'. Once you've generated the new set of variables, find the most correlated (in absolute value) one with is_positive_growth_5d_future and round it to 3 digits after the comma.\n",
    "\n",
    "NOTE: new dummies will be used as features in the next tasks, please leave them in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Fin Data Sources\n",
    "import yfinance as yf\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "#Data viz\n",
    "import plotly.graph_objs as go\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "# for graphs\n",
    "import matplotlib.pyplot as plt\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_parquet(\"stocks_df_combined_2024_05_07.parquet.brotli\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full[df_full.Date>='2000-01-01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHLCV = ['Open','High','Low','Close','Adj Close_x','Volume']\n",
    "\n",
    "CATEGORICAL = ['Month', 'Weekday', 'Ticker', 'ticker_type']\n",
    "\n",
    "TO_PREDICT = [g for g in df_full.keys() if (g.find('future')>=0)]\n",
    "\n",
    "TO_DROP = ['Year','Date','index_x', 'index_y', 'index', 'Quarter','Adj Close_y'] + CATEGORICAL + OHLCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8166/1818720232.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  df['ln_volume'] = df.Volume.apply(lambda x: np.log(x))\n",
      "/tmp/ipykernel_8166/1818720232.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['ln_volume'] = df.Volume.apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "df['ln_volume'] = df.Volume.apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical patterns count = 61, examples = ['cdl2crows', 'cdl3blackrows', 'cdl3inside', 'cdl3linestrike', 'cdl3outside']\n"
     ]
    }
   ],
   "source": [
    " # manually defined features\n",
    "GROWTH = [g for g in df_full.keys() if (g.find('growth_')==0)&(g.find('future')<0)]\n",
    "GROWTH\n",
    "\n",
    "CUSTOM_NUMERICAL = ['SMA10', 'SMA20', 'growing_moving_average', 'high_minus_low_relative','volatility', 'ln_volume']\n",
    "     \n",
    "\n",
    "TECHNICAL_INDICATORS = ['adx', 'adxr', 'apo', 'aroon_1','aroon_2', 'aroonosc',\n",
    " 'bop', 'cci', 'cmo','dx', 'macd', 'macdsignal', 'macdhist', 'macd_ext',\n",
    " 'macdsignal_ext', 'macdhist_ext', 'macd_fix', 'macdsignal_fix',\n",
    " 'macdhist_fix', 'mfi', 'minus_di', 'mom', 'plus_di', 'dm', 'ppo',\n",
    " 'roc', 'rocp', 'rocr', 'rocr100', 'rsi', 'slowk', 'slowd', 'fastk',\n",
    " 'fastd', 'fastk_rsi', 'fastd_rsi', 'trix', 'ultosc', 'willr',\n",
    " 'ad', 'adosc', 'obv', 'atr', 'natr', 'ht_dcperiod', 'ht_dcphase',\n",
    " 'ht_phasor_inphase', 'ht_phasor_quadrature', 'ht_sine_sine', 'ht_sine_leadsine',\n",
    " 'ht_trendmod', 'avgprice', 'medprice', 'typprice', 'wclprice']\n",
    "\n",
    "TECHNICAL_PATTERNS = [g for g in df_full.keys() if g.find('cdl')>=0]\n",
    "print(f'Technical patterns count = {len(TECHNICAL_PATTERNS)}, examples = {TECHNICAL_PATTERNS[0:5]}')\n",
    "\n",
    "MACRO = ['gdppot_us_yoy', 'gdppot_us_qoq', 'cpi_core_yoy', 'cpi_core_mom', 'FEDFUNDS',\n",
    " 'DGS1', 'DGS5', 'DGS10']\n",
    "NUMERICAL = GROWTH + TECHNICAL_INDICATORS + TECHNICAL_PATTERNS + CUSTOM_NUMERICAL + MACRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8166/2159284025.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['January' 'January' 'January' ... 'May' 'May' 'May']' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:,'Month'] = df.Month.dt.strftime('%B')\n",
      "/tmp/ipykernel_8166/2159284025.py:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0' '1' '2' ... '4' '0' '1']' has dtype incompatible with int32, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:,'Weekday'] = df.Weekday.astype(str)\n"
     ]
    }
   ],
   "source": [
    "df.loc[:,'Month'] = df.Month.dt.strftime('%B')\n",
    "df.loc[:,'Weekday'] = df.Weekday.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8166/1038171603.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['month_wom'] = df.apply(lambda row: f'{row[\"Month\"]}-{(row[\"Date\"].day-1)//7+1}', axis=1)\n"
     ]
    }
   ],
   "source": [
    "df['month_wom'] = df.apply(lambda row: f'{row[\"Month\"]}-{(row[\"Date\"].day-1)//7+1}', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3490    January-1\n",
       "3491    January-1\n",
       "3492    January-1\n",
       "3493    January-1\n",
       "3494    January-1\n",
       "          ...    \n",
       "5422      April-5\n",
       "5423        May-1\n",
       "5424        May-1\n",
       "5425        May-1\n",
       "5426        May-1\n",
       "Name: month_wom, Length: 182675, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['month_wom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL.append('month_wom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_variables = pd.get_dummies(df[CATEGORICAL], dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 115 dummy vars\n"
     ]
    }
   ],
   "source": [
    "print(f'We have {len(dummy_variables.columns)} dummy vars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMIES = dummy_variables.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dummies = pd.concat([df, dummy_variables], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_corr = df_with_dummies[DUMMIES].corrwith(df['is_positive_growth_5d_future'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month_wom_September-3    0.034537\n",
       "month_wom_March-4        0.026058\n",
       "month_wom_October-5      0.026023\n",
       "month_wom_February-3     0.024578\n",
       "month_wom_May-4          0.022264\n",
       "                           ...   \n",
       "Ticker_RELIANCE.NS       0.000106\n",
       "Ticker_TTE               0.000063\n",
       "month_wom_February-1     0.000059\n",
       "month_wom_March-2        0.000036\n",
       "month_wom_November-1     0.000033\n",
       "Length: 115, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_corr.apply(np.abs).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest value is 0.03453687650579432\n"
     ]
    }
   ],
   "source": [
    "print(f'The highest value is {df_with_corr.apply(np.abs).sort_values(ascending=False).iloc[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: \n",
    "* You're asked to define two new 'hand' rules (leading to 'positive' subtrees):\n",
    "\n",
    "* pred3_manual_gdp_fastd: (gdppot_us_yoy <= 0.027) & (fastd >= 0.251)\n",
    "* pred4_manual_gdp_wti_oil: (gdppot_us_yoy >= 0.027) & (growth_wti_oil_30d <= 1.005)\n",
    "* Extend the Code Snippet 3 (Manual \"hand rule\" predictions): Calculate and add new rules (pred3 and pred4) to the dataframe.You should notice that one of the predictions doesn't have any positive predictions on TEST dataset (while it has many on TRAIN+VALIDATION).\n",
    "\n",
    "* Debug: check in the new_df and the original dataset/data generation process that we didn't make any mistakes during the data transformation step.\n",
    "\n",
    "* Explain why this can happen even if there are no errors in the data features.\n",
    "\n",
    "* As a result, write down the precision score for the remaining predictor (round to three decimal points). E.g. if you have 0.57897, your answer should be 0.579."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_split(df, min_date, max_date, train_prop=0.7, val_prop=0.15, test_prop=0.15):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into three buckets based on the temporal order of the 'Date' column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame to split.\n",
    "        min_date (str or Timestamp): Minimum date in the DataFrame.\n",
    "        max_date (str or Timestamp): Maximum date in the DataFrame.\n",
    "        train_prop (float): Proportion of data for training set (default: 0.6).\n",
    "        val_prop (float): Proportion of data for validation set (default: 0.2).\n",
    "        test_prop (float): Proportion of data for test set (default: 0.2).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The input DataFrame with a new column 'split' indicating the split for each row.\n",
    "    \"\"\"\n",
    "    # Define the date intervals\n",
    "    train_end = min_date + pd.Timedelta(days=(max_date - min_date).days * train_prop)\n",
    "    val_end = train_end + pd.Timedelta(days=(max_date - min_date).days * val_prop)\n",
    "\n",
    "    # Assign split labels based on date ranges\n",
    "    split_labels = []\n",
    "    for date in df['Date']:\n",
    "        if date <= train_end:\n",
    "            split_labels.append('train')\n",
    "        elif date <= val_end:\n",
    "            split_labels.append('validation')\n",
    "        else:\n",
    "            split_labels.append('test')\n",
    "\n",
    "    # Add 'split' column to the DataFrame\n",
    "    df['split'] = split_labels\n",
    "\n",
    "    return df\n",
    "\n",
    "min_date_df = df_with_dummies.Date.min()\n",
    "max_date_df = df_with_dummies.Date.max()\n",
    "\n",
    "df_with_dummies = temporal_split(df_with_dummies,\n",
    "                                 min_date = min_date_df,\n",
    "                                 max_date = max_date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate manual predictions\n",
    "# Let's label all prediction features with prefix \"pred\"\n",
    "new_df = df_with_dummies.copy()\n",
    "new_df['pred0_manual_cci'] = (new_df.cci>200).astype(int)\n",
    "new_df['pred1_manual_prev_g1'] = (new_df.growth_1d>1).astype(int)\n",
    "new_df['pred2_manual_prev_g1_and_snp'] = ((new_df['growth_1d'] > 1) & (new_df['growth_snp500_1d'] > 1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['pred3_manual_gdp_fastd'] = ((new_df['gdppot_us_yoy'] <= 0.027) & (new_df['fastd'] >= 0.251)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['pred4_manual_gdp_wti_oil']=((new_df['gdppot_us_yoy']>=0.027) & (new_df['growth_wti_oil_30d'] <= 1.005)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_thumb_prediction = new_df[new_df[\"split\"] == 'test'][['pred4_manual_gdp_wti_oil', 'pred3_manual_gdp_fastd', 'is_positive_growth_5d_future']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred4_manual_gdp_wti_oil    1\n",
       "pred3_manual_gdp_fastd      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_thumb_prediction[['pred4_manual_gdp_wti_oil', 'pred3_manual_gdp_fastd']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([1, 0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_thumb_prediction['pred4_manual_gdp_wti_oil'].unique(),  rule_thumb_prediction['pred3_manual_gdp_fastd'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.555\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy {round((rule_thumb_prediction[\"pred3_manual_gdp_fastd\"]==rule_thumb_prediction[\"is_positive_growth_5d_future\"]).sum()/len(rule_thumb_prediction),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (1 point): Unique correct predictions from a 10-levels deep Decision Tree Classifier (pred5_clf_10)\n",
    "\n",
    "* What is the total number of records in the TEST dataset when the new prediction pred5_clf_10 is better than all 'hand' rules (pred0..pred4)?\n",
    "\n",
    "NOTE: please include random_state=42 to Decision Tree Classifier init function (line clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)) to ensure everyone gets the same results.\n",
    "\n",
    "Suggested solution:\n",
    "\n",
    "Step1: Rewrite the '1.4.3 Inference for a decision tree' piece for the Decision Tree Classifier with max_depth=10 (clf_10), so that you fit the model on TRAIN+VALIDATION sets (unchanged from the lecture), but predict on the whole set X_all (to be able to define a new column 'pred5_clf_10' in the dataframe new_df). Here is the link with explanation. It will solve the problem in 1.4.5 when predictions were made only for Test dataset and couldn't be easily joined with the full dataset.\n",
    "\n",
    "Step2: Once you have it, define a new column 'only_pred5_is_correct' similar to 'hand' prediction rules with several conditions: is_positive_growth_5d_future AND is_correct_pred5 should be equal 1, while all other predictions is_correct_pred0..is_correct_pred4 should be equal to 0.\n",
    "\n",
    "Step3: Convert 'only_pred5_is_correct' column from bool to int, and find how many times it is equal to 1 in the TEST set. Write down this as an answer.\n",
    "\n",
    "ADVANCED: define a function that can be applied to the whole row of predictions (a few examples of pandas-apply-row-functions) and can find whether some prediction 'predX' (where X is one of the predictions) is uniquely correct. It should work even if there are 100 predictions available, so that you don't define manually the condition for 'predX'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def remove_infinite_values(X):\n",
    "    \"\"\"\n",
    "    Remove infinite values from the input array.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input array (NumPy array or array-like)\n",
    "\n",
    "    Returns:\n",
    "    - Array with infinite values removed\n",
    "    \"\"\"\n",
    "    return X[np.isfinite(X).all(axis=1)]\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X is your input data\n",
    "# filtered_X = remove_infinite_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Month_April',\n",
       " 'Month_August',\n",
       " 'Month_December',\n",
       " 'Month_February',\n",
       " 'Month_January',\n",
       " 'Month_July',\n",
       " 'Month_June',\n",
       " 'Month_March',\n",
       " 'Month_May',\n",
       " 'Month_November',\n",
       " 'Month_October',\n",
       " 'Month_September',\n",
       " 'Weekday_0',\n",
       " 'Weekday_1',\n",
       " 'Weekday_2',\n",
       " 'Weekday_3',\n",
       " 'Weekday_4',\n",
       " 'Weekday_5',\n",
       " 'Weekday_6',\n",
       " 'Ticker_AAPL',\n",
       " 'Ticker_ACN',\n",
       " 'Ticker_AMZN',\n",
       " 'Ticker_ASML',\n",
       " 'Ticker_AVGO',\n",
       " 'Ticker_BHARTIARTL.NS',\n",
       " 'Ticker_BRK-B',\n",
       " 'Ticker_CDI.PA',\n",
       " 'Ticker_GOOG',\n",
       " 'Ticker_HDB',\n",
       " 'Ticker_HINDUNILVR.NS',\n",
       " 'Ticker_IBN',\n",
       " 'Ticker_IDEXY',\n",
       " 'Ticker_INFY',\n",
       " 'Ticker_ITC.NS',\n",
       " 'Ticker_JPM',\n",
       " 'Ticker_LICI.NS',\n",
       " 'Ticker_LLY',\n",
       " 'Ticker_LT.NS',\n",
       " 'Ticker_MC.PA',\n",
       " 'Ticker_META',\n",
       " 'Ticker_MSFT',\n",
       " 'Ticker_NVDA',\n",
       " 'Ticker_NVO',\n",
       " 'Ticker_OR.PA',\n",
       " 'Ticker_RELIANCE.NS',\n",
       " 'Ticker_RMS.PA',\n",
       " 'Ticker_SAP',\n",
       " 'Ticker_SBIN.NS',\n",
       " 'Ticker_SIE.DE',\n",
       " 'Ticker_TCS.NS',\n",
       " 'Ticker_TTE',\n",
       " 'Ticker_V',\n",
       " 'ticker_type_EU',\n",
       " 'ticker_type_INDIA',\n",
       " 'ticker_type_US',\n",
       " 'month_wom_April-1',\n",
       " 'month_wom_April-2',\n",
       " 'month_wom_April-3',\n",
       " 'month_wom_April-4',\n",
       " 'month_wom_April-5',\n",
       " 'month_wom_August-1',\n",
       " 'month_wom_August-2',\n",
       " 'month_wom_August-3',\n",
       " 'month_wom_August-4',\n",
       " 'month_wom_August-5',\n",
       " 'month_wom_December-1',\n",
       " 'month_wom_December-2',\n",
       " 'month_wom_December-3',\n",
       " 'month_wom_December-4',\n",
       " 'month_wom_December-5',\n",
       " 'month_wom_February-1',\n",
       " 'month_wom_February-2',\n",
       " 'month_wom_February-3',\n",
       " 'month_wom_February-4',\n",
       " 'month_wom_February-5',\n",
       " 'month_wom_January-1',\n",
       " 'month_wom_January-2',\n",
       " 'month_wom_January-3',\n",
       " 'month_wom_January-4',\n",
       " 'month_wom_January-5',\n",
       " 'month_wom_July-1',\n",
       " 'month_wom_July-2',\n",
       " 'month_wom_July-3',\n",
       " 'month_wom_July-4',\n",
       " 'month_wom_July-5',\n",
       " 'month_wom_June-1',\n",
       " 'month_wom_June-2',\n",
       " 'month_wom_June-3',\n",
       " 'month_wom_June-4',\n",
       " 'month_wom_June-5',\n",
       " 'month_wom_March-1',\n",
       " 'month_wom_March-2',\n",
       " 'month_wom_March-3',\n",
       " 'month_wom_March-4',\n",
       " 'month_wom_March-5',\n",
       " 'month_wom_May-1',\n",
       " 'month_wom_May-2',\n",
       " 'month_wom_May-3',\n",
       " 'month_wom_May-4',\n",
       " 'month_wom_May-5',\n",
       " 'month_wom_November-1',\n",
       " 'month_wom_November-2',\n",
       " 'month_wom_November-3',\n",
       " 'month_wom_November-4',\n",
       " 'month_wom_November-5',\n",
       " 'month_wom_October-1',\n",
       " 'month_wom_October-2',\n",
       " 'month_wom_October-3',\n",
       " 'month_wom_October-4',\n",
       " 'month_wom_October-5',\n",
       " 'month_wom_September-1',\n",
       " 'month_wom_September-2',\n",
       " 'month_wom_September-3',\n",
       " 'month_wom_September-4',\n",
       " 'month_wom_September-5']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DUMMIES.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: X_train (152846, 302),  X_test (29829, 302)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets based on the split date\n",
    "features_list = NUMERICAL+DUMMIES.to_list()\n",
    "to_predict = 'is_positive_growth_5d_future'\n",
    "\n",
    "train_df = new_df[new_df.split.isin(['train','validation'])].copy(deep=True)\n",
    "test_df = new_df[new_df.split.isin(['test'])].copy(deep=True)\n",
    "\n",
    "# ONLY numerical Separate features and target variable for training and testing sets\n",
    "# need Date and Ticker later when merging predictions to the dataset\n",
    "X_train = train_df[features_list+[to_predict,'Date','Ticker']]\n",
    "X_test = test_df[features_list+[to_predict,'Date','Ticker']]\n",
    "\n",
    "print(f'length: X_train {X_train.shape},  X_test {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: X_train_imputed (152846, 302),  X_test_imputed (29829, 302)\n"
     ]
    }
   ],
   "source": [
    "# Disable SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Need to fill NaNs somehow\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "\n",
    "print(f'length: X_train_imputed {X_train.shape},  X_test_imputed {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[to_predict]\n",
    "y_test = X_test[to_predict]\n",
    "\n",
    "# remove y_train, y_test from X_ dataframes\n",
    "del X_train[to_predict]\n",
    "del X_test[to_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "def fit_decision_tree(X, y, max_depth=20):\n",
    "# Initialize the Decision Tree Classifier\n",
    "  clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "\n",
    "  # Fit the classifier to the training data\n",
    "  clf.fit(X, y)\n",
    "  return clf, X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_10, train_columns = fit_decision_tree(X=X_train.drop(['Date','Ticker'],axis=1),\n",
    "                           y=y_train,\n",
    "                           max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['pred5_clf10'] = clf_10.predict(new_df[features_list].replace([np.inf, -np.inf], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pred0_manual_cci',\n",
       " 'pred1_manual_prev_g1',\n",
       " 'pred2_manual_prev_g1_and_snp',\n",
       " 'pred3_manual_gdp_fastd',\n",
       " 'pred4_manual_gdp_wti_oil']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_columns = [_pred  for _pred in new_df.columns if 'pred' in _pred]\n",
    "pred_columns.remove( 'pred5_clf10')\n",
    "pred_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def only_tree_is_correct(row):\n",
    "    if (row['pred5_clf10']!=1) and (row['is_positive_growth_5d_future']!=1):\n",
    "        return False\n",
    "    for _pred_col in pred_columns:\n",
    "        if row[_pred_col]==1:\n",
    "            return False\n",
    "    \n",
    "    return True    \n",
    "\n",
    "new_df[new_df.split == 'test'].apply(only_tree_is_correct, axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: (2 points) Hyperparameter tuning for a Decision Tree\n",
    "What is the optimal tree depth (from 1 to 20) for a DecisionTreeClassifier?\n",
    "\n",
    "NOTE: please include random_state=42 to Decision Tree Classifier init function (line clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)) to ensure consistency in results.\n",
    "\n",
    "Follow these steps to find the optimal max_depth:\n",
    "\n",
    "Iterate through max_depth values from 1 to 20.\n",
    "Train the Decision Tree Classifier with the current max_depth parameter.\n",
    "Optionally, visualize how the 'head' of each fitted tree changes with more advanced (=deep) trees. You can use the sklearn.tree.plot_tree() function, or the compact way with the export_text() functionality (Stack Overflow example):\n",
    "from sklearn.tree import export_text\n",
    "tree_rules = export_text(model, feature_names=list(X_train), max_depth=3)\n",
    "print(tree_rules)\n",
    "Calculate the precision score (you can use the function sklearn.metrics.precision_score()) on the TEST dataset for each of the fitted trees. You can also compare it with the precision score on a VALIDATION dataset, which is included to the training phase (to have more data to train on). You should see that the precision score on a VALIDATION set starts to grow with the complexity of a tree (overfit), which isn't seen on the precision score on TEST.\n",
    "Identify the optimal max_depth, where the precision score is the highest on the TEST dataset. Record this value as best_max_depth and submit as an answer.\n",
    "Make predictions on all records (TRAIN+VALIDATION+TEST) and add the new prediction pred6_clf_best to the dataframe new_df.\n",
    "Additionally, compare the precision score of the tuned decision tree with previous predictions. You should observe an improvement (>0.58, or more than 58% precision), indicating that the tuned tree outperforms previous manual \"hand\" rules and Decision Tree predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tree with depth 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 0.5551979617151095\n",
      "Accuracy on validation: 0.5717299578059072\n",
      "Training tree with depth 2\n",
      "Accuracy on test: 0.5553655838278185\n",
      "Accuracy on validation: 0.5717299578059072\n",
      "Training tree with depth 3\n",
      "Accuracy on test: 0.5551979617151095\n",
      "Accuracy on validation: 0.5696883081529877\n",
      "Training tree with depth 4\n",
      "Accuracy on test: 0.5552650105601931\n",
      "Accuracy on validation: 0.5696883081529877\n",
      "Training tree with depth 5\n",
      "Accuracy on test: 0.5556002547856114\n",
      "Accuracy on validation: 0.5696883081529877\n",
      "Training tree with depth 6\n",
      "Accuracy on test: 0.5677696201682926\n",
      "Accuracy on validation: 0.567442493534776\n",
      "Training tree with depth 7\n",
      "Accuracy on test: 0.5650206175198632\n",
      "Accuracy on validation: 0.5721382877364911\n",
      "Training tree with depth 8\n",
      "Accuracy on test: 0.5650206175198632\n",
      "Accuracy on validation: 0.5760514495712535\n",
      "Training tree with depth 9\n",
      "Accuracy on test: 0.5669985584498307\n",
      "Accuracy on validation: 0.5781951817068192\n",
      "Training tree with depth 10\n",
      "Accuracy on test: 0.5570418049549096\n",
      "Accuracy on validation: 0.5858513679052675\n",
      "Training tree with depth 11\n",
      "Accuracy on test: 0.5516778973482181\n",
      "Accuracy on validation: 0.6065400843881856\n",
      "Training tree with depth 12\n",
      "Accuracy on test: 0.5465821851218613\n",
      "Accuracy on validation: 0.6267524159520893\n",
      "Training tree with depth 13\n",
      "Accuracy on test: 0.5485936504743706\n",
      "Accuracy on validation: 0.6341023547025997\n",
      "Training tree with depth 14\n",
      "Accuracy on test: 0.5470515270374467\n",
      "Accuracy on validation: 0.6489723696746972\n",
      "Training tree with depth 15\n",
      "Accuracy on test: 0.5597237587582554\n",
      "Accuracy on validation: 0.6660201442765755\n",
      "Training tree with depth 16\n",
      "Accuracy on test: 0.540246069261457\n",
      "Accuracy on validation: 0.6857560909214645\n",
      "Training tree with depth 17\n",
      "Accuracy on test: 0.5368601025847329\n",
      "Accuracy on validation: 0.6995372260786715\n",
      "Training tree with depth 18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m21\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining tree with depth \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     clf, train_columns \u001b[38;5;241m=\u001b[39m \u001b[43mfit_decision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTicker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                           \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy on test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(y_test,\u001b[38;5;250m \u001b[39mclf\u001b[38;5;241m.\u001b[39mpredict(X_test\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m\"\u001b[39m])))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m     y_val \u001b[38;5;241m=\u001b[39m new_df[new_df\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m][to_predict]\n",
      "Cell \u001b[0;32mIn[52], line 8\u001b[0m, in \u001b[0;36mfit_decision_tree\u001b[0;34m(X, y, max_depth)\u001b[0m\n\u001b[1;32m      5\u001b[0m clf \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39mmax_depth, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Fit the classifier to the training data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clf, X\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/stock-markets-analytics-zoomcamp-IIo2NssN/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/stock-markets-analytics-zoomcamp-IIo2NssN/lib/python3.10/site-packages/sklearn/tree/_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/stock-markets-analytics-zoomcamp-IIo2NssN/lib/python3.10/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "trees_result = defaultdict(int)\n",
    "for k in range(1, 21):\n",
    "    print(f'Training tree with depth {k}')\n",
    "    clf, train_columns = fit_decision_tree(X=X_train.drop(['Date','Ticker'],axis=1),\n",
    "                           y=y_train,\n",
    "                           max_depth=k)\n",
    "    print(f'Accuracy on test: {precision_score(y_test, clf.predict(X_test.drop(columns=[\"Date\", \"Ticker\"])))}')\n",
    "    \n",
    "    trees_result[k] = precision_score(y_test, clf.predict(X_test.drop(columns=[\"Date\", \"Ticker\"])))\n",
    "    \n",
    "    y_val = new_df[new_df.split == 'validation'][to_predict]\n",
    "    X_val = new_df[new_df.split == 'validation'][features_list].replace([np.inf, -np.inf], 0)\n",
    "    print(f'Accuracy on validation: {precision_score(y_val, clf.predict(X_val))}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-markets-analytics-zoomcamp-IIo2NssN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
